{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from colorama import Fore,Back\n",
    "import csv\n",
    "from pyarabic.araby import TASHKEEL \n",
    "import time\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "# Please consider running all cells in order . some cells depend on previous ones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helping functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from termcolor import colored\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def extract_arabic_alphabits(text : str)->list[str]:\n",
    "    \"\"\"returns Arabic alphabets keeping diacritics\n",
    "\n",
    "    Args:\n",
    "        text (str): text we need to split\n",
    "\n",
    "    Returns:\n",
    "        list[str]: list of alphabets with diacritics if found\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for word in text.split():\n",
    "        for char in word:\n",
    "            if char not in TASHKEEL:\n",
    "                res.append(char)\n",
    "            else:\n",
    "                res[-1] = res[-1]+char\n",
    "                \n",
    "    return res\n",
    "            \n",
    "            \n",
    "def calculate_cer_and_display(reference: str, hypothesis: str) -> Tuple[float, str]:\n",
    "    \"\"\"calculate character error rate between reference and hypothesis\n",
    "    \n",
    "\n",
    "    Args:\n",
    "        reference (str): string to calcaute cer based on\n",
    "        hypothesis (str): string to treat as input (if we consider refrence as constant)\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, str]: float representing cer , and str representing feedback\n",
    "    \"\"\"\n",
    "    \n",
    "    reference = extract_arabic_alphabits(reference)\n",
    "    hypothesis = extract_arabic_alphabits(hypothesis)\n",
    "\n",
    "    m, n = len(reference), len(hypothesis)\n",
    "    dp = np.zeros((m + 1, n + 1), dtype=int)\n",
    "\n",
    "    for i in range(m + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(n + 1):\n",
    "        dp[0][j] = j\n",
    "\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if reference[i - 1] == hypothesis[j - 1]:\n",
    "                dp[i][j] = dp[i - 1][j - 1]\n",
    "            else:\n",
    "                dp[i][j] = min(dp[i - 1][j] + 1,   \n",
    "                                dp[i][j - 1] + 1,   \n",
    "                                dp[i - 1][j - 1] + 1)  \n",
    "\n",
    "    i, j = m, n\n",
    "    result = []\n",
    "\n",
    "    while i > 0 or j > 0:\n",
    "        if i > 0 and j > 0 and reference[i - 1] == hypothesis[j - 1]:\n",
    "            result.append(colored(reference[i - 1], 'green'))\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "        elif i > 0 and (j == 0 or dp[i][j] == dp[i - 1][j] + 1):\n",
    "            result.append(colored(reference[i - 1], 'red'))  \n",
    "            i -= 1\n",
    "        elif j > 0 and (i == 0 or dp[i][j] == dp[i][j - 1] + 1):\n",
    "            result.append(colored(hypothesis[j - 1], 'red'))  \n",
    "            j -= 1\n",
    "        else:\n",
    "            result.append(colored(reference[i - 1], 'red') + '/' + colored(hypothesis[j - 1], 'red'))  \n",
    "            i -= 1\n",
    "            j -= 1\n",
    "\n",
    "    result.reverse()\n",
    "    cer = dp[m][n] / m\n",
    "    return cer, ''.join(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syllables recitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Al-Fatiha , This surah is created with Al-Hossary voice reciting al-Fatiha and passed on 'base' ASR model\n",
    "moshaf_syl = []\n",
    "with open('./Moshaf/SYL_AlFaiha_Hosary.csv' ,'r', encoding='utf-8') as fp:\n",
    "    reader = csv.reader(fp)\n",
    "    for line in reader:\n",
    "        moshaf_syl.append(*line)\n",
    "moshaf_syl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def syl_request(audiopth):\n",
    "    \"\"\"makes request to 'SYL' endpoint , to get ASR_syl service on a given audio file\n",
    "\n",
    "    Args:\n",
    "        audiopth (str , path like): path to audio file\n",
    "\n",
    "    Returns:\n",
    "        dict : server response\n",
    "    \"\"\"\n",
    "    url = 'http://127.0.0.1:8000/SYL'\n",
    "    file = {'file': open(audiopth, 'rb')}\n",
    "    resp = requests.post(url=url, files=file) \n",
    "    return resp\n",
    "def syl_recitation(audiopth,verse_number):\n",
    "    \"\"\" performs recitation based on syllables , given audio file of recited verse and index of this specific verse.\n",
    "    this function calls 'syl_request' function\n",
    "\n",
    "    Args:\n",
    "        audiopth (str , path like): path to audio file\n",
    "        verse_number (int): index of verse \n",
    "    \"\"\"\n",
    "    resp=syl_request(audiopth)\n",
    "    hypothesis = resp.json()['message']\n",
    "    idx = verse_number-1\n",
    "    reference = moshaf_syl[idx]\n",
    "    \n",
    "    cer, result_str = calculate_cer_and_display(reference, hypothesis)\n",
    "    print(f\"CER: {cer:.2f}\")\n",
    "    print(result_str)\n",
    "    return(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audiopth = r\"./testcases/syllables_wrong_testcases/002_1.wav\"\n",
    "verse_number = 2\n",
    "syl_recitation(audiopth,verse_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell utilizes previous funtions to perform recitation (focusng on syllables ) with mic instead of audio file.\n",
    "\n",
    "import pyaudio\n",
    "from collections import deque\n",
    "import soundfile as sf\n",
    "import os\n",
    "\n",
    "FORMAT = pyaudio.paFloat32  \n",
    "CHANNELS = 1  \n",
    "RATE = 16000  \n",
    "CHUNK = 1600  \n",
    "p = pyaudio.PyAudio()\n",
    "input_stream = p.open(format=FORMAT,\n",
    "                        channels=CHANNELS,\n",
    "                        rate=RATE,\n",
    "                        input=True,\n",
    "                        frames_per_buffer=CHUNK) \n",
    "\n",
    "#every 10 chunks equals 1 second , 35 chunks equals 3.5 seconds\n",
    "MAX_LEN=35\n",
    "que = deque(maxlen=MAX_LEN)\n",
    "\n",
    "print(Fore.YELLOW,Back.GREEN , 'Recording...' , Fore.RESET,Back.RESET)\n",
    "while len(que) != MAX_LEN:\n",
    "    data = input_stream.read(CHUNK)\n",
    "    que.append(data)\n",
    "else:\n",
    "    \n",
    "    arr = np.frombuffer(b''.join(que),dtype=np.float32)\n",
    "    audio_output_path = './syl_audio_output_sample.wav'\n",
    "    sf.write(audio_output_path,arr,16_000)\n",
    "    print(Fore.BLUE,Back.GREEN , 'Uploading...' , Fore.RESET,Back.RESET)\n",
    "    syl_recitation(audio_output_path,2)\n",
    "    os.remove(audio_output_path)\n",
    "    \n",
    "input_stream.stop_stream()\n",
    "input_stream.close()\n",
    "p.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASR recitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ASR_request(pth , modelType='tiny'):\n",
    "    \"\"\"makes request to 'tiny' or 'base'  endpoint , to get ASR service on a given audio file\n",
    "\n",
    "    Args:\n",
    "        audiopth (str , path like): path to audio file\n",
    "\n",
    "    Returns:\n",
    "        dict : server response\n",
    "    \"\"\"\n",
    "    \n",
    "    url = f'http://127.0.0.1:8000/ASR?ASR_type={modelType}'   #chose base or tiny. server defualt is tiny.\n",
    "    file = {'file': open(pth, 'rb')}\n",
    "    resp = requests.post(url=url, files=file) \n",
    "    return(resp)    \n",
    "\n",
    "ASR_request('./testcases/X2/11.wav' , 'base').json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare requirements (loading surah , feedback functions) \n",
    "surah_que_main = deque()\n",
    "with open ('./Moshaf/ASR_AlFaiha_Hosary.csv' , 'r' , encoding='utf-8') as fp:\n",
    "    reader = csv.reader(fp)\n",
    "    for line in reader:\n",
    "        surah_que_main.append(*line)\n",
    "        \n",
    "\n",
    "\n",
    "def print_surah(word_limit=10):\n",
    "    \"\"\"prints feedback in terminal \n",
    "\n",
    "    Args:\n",
    "        word_limit (int): number of words in each line . Defaults to 10.\n",
    "    \"\"\"\n",
    "    current_line=[]\n",
    "    for word in feedBack:\n",
    "        current_line.append(word)\n",
    "        if len(current_line) == word_limit:\n",
    "            print(' '.join(current_line))\n",
    "            current_line = []\n",
    "            \n",
    "    if current_line:\n",
    "            print(' '.join(current_line))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes feedback in terminal. red words not pronounced. Green words pronounced. \n",
    "# prints recorded audio and also gives feedback on it \n",
    "# prints hint of word to say. recording and uploading indicator\n",
    "\n",
    "surah_que=surah_que_main.copy()\n",
    "feedBack = [colored(i,'red')for i in surah_que_main]\n",
    "\n",
    "\n",
    "p = pyaudio.PyAudio()\n",
    "input_stream = p.open(format=FORMAT,\n",
    "                        channels=CHANNELS,\n",
    "                        rate=RATE,\n",
    "                        input=True,\n",
    "                        frames_per_buffer=CHUNK)    \n",
    "MAX_LEN=30\n",
    "audio_surah_que = deque(maxlen=MAX_LEN)\n",
    "\n",
    "display(print_surah())\n",
    "try:\n",
    "    feedBack_IDX= 0 \n",
    "    while len(surah_que)!=0:\n",
    "        \n",
    "        #display_handle1 = display(output1, display_id=True)\n",
    "        #update_recodring('r')\n",
    "        print(Fore.GREEN, 'Recording...' , Fore.RESET,Back.RESET , end='\\r')\n",
    "        while len(audio_surah_que) != MAX_LEN:\n",
    "            data = input_stream.read(CHUNK)\n",
    "            audio_surah_que.append(data)\n",
    "        else:\n",
    "            arr = np.frombuffer(b''.join(audio_surah_que),dtype=np.float32)\n",
    "            audio_output_path = './surah_audio_output_sample.wav'\n",
    "            sf.write(audio_output_path,arr,16_000)\n",
    "            print(Fore.RED, 'Uploading...' , Fore.RESET,Back.RESET,end='\\r')\n",
    "            #update_recodring('u')\n",
    "            response = ASR_request(audio_output_path,'base')\n",
    "            os.remove(audio_output_path)\n",
    "            audio_surah_que.clear()\n",
    "            \n",
    "            resp = response.json()['message']\n",
    "            resp= resp.split(' ')\n",
    "            result = []\n",
    "            \n",
    "            for word in resp:\n",
    "                \n",
    "                if word == surah_que[0]:\n",
    "                    feedBack[feedBack_IDX] = colored(surah_que[0],'green')\n",
    "                    feedBack_IDX+=1\n",
    "                    result.append(colored((word),'green'))\n",
    "                    surah_que.popleft()\n",
    "                else:\n",
    "                    result.append(colored((word),'red'))\n",
    "            \n",
    "            if len(surah_que)==0:\n",
    "                clear_output(wait=True)\n",
    "                display(print_surah(),print('\\n',(' '.join(result))))\n",
    "                raise KeyboardInterrupt\n",
    "                \n",
    "            clear_output(wait=True)\n",
    "            display(print_surah(),(surah_que[0]) ,print('\\n',(' '.join(result))))\n",
    "            #time.sleep(0.1)\n",
    "except KeyboardInterrupt:\n",
    "    input_stream.stop_stream()\n",
    "    input_stream.close()\n",
    "    p.terminate()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KWS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def KWS_request(pth , modelType='CNN'):\n",
    "    \"\"\"Performing KWS service. through sending request to kws endpoing. it's either 'CNN' or 'LSTM' \n",
    "\n",
    "    Args:\n",
    "        pth (str , path like): path to audio file to perform KWS on\n",
    "        modelType (str, optional): KWS type. Defaults to 'CNN'.\n",
    "    \"\"\"\n",
    "    url = f'http://127.0.0.1:8000/KWS?KWS_type={modelType}'   \n",
    "    file = {'file': open(pth, 'rb')}\n",
    "    resp = requests.post(url=url, files=file) \n",
    "    return(resp.json())    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell records from user his recitation , and gives feedback in terminal.\n",
    "# prints feedback in red, green and percentages.\n",
    "# audio length 2.5 seconds \n",
    "\n",
    "KWS_Surah = []\n",
    "for i in range (4,8):\n",
    "    KWS_Surah.append(colored(surah_que_main[i],'red'))\n",
    "print(' '.join(KWS_Surah))\n",
    "\n",
    "p = pyaudio.PyAudio()\n",
    "input_stream = p.open(format=FORMAT,\n",
    "                        channels=CHANNELS,\n",
    "                        rate=RATE,\n",
    "                        input=True,\n",
    "                        frames_per_buffer=CHUNK)    \n",
    "MAX_LEN=25\n",
    "audio_surah_que = deque(maxlen=MAX_LEN)\n",
    "\n",
    "try:\n",
    "    counter = 0\n",
    "    recitated_word_counter=4\n",
    "    while True:\n",
    "        print(Fore.GREEN, 'Recording...' , Fore.RESET,Back.RESET )\n",
    "        while len(audio_surah_que) != MAX_LEN:\n",
    "            data = input_stream.read(CHUNK)\n",
    "            audio_surah_que.append(data)\n",
    "        else:\n",
    "            arr = np.frombuffer(b''.join(audio_surah_que),dtype=np.float32)\n",
    "            audio_output_path = './surah_audio_output_sample.wav'\n",
    "            sf.write(audio_output_path,arr,16_000)\n",
    "            response = KWS_request(audio_output_path,'CNN')\n",
    "            os.remove(audio_output_path)\n",
    "            audio_surah_que.clear()\n",
    "            idx=np.array(response['message']).argmax()\n",
    "            \n",
    "            if idx == recitated_word_counter and response['message'][idx] > 0.7:\n",
    "                \n",
    "                KWS_Surah[recitated_word_counter-4] = colored(surah_que_main[idx],'green')\n",
    "                recitated_word_counter+=1\n",
    "            \n",
    "            clear_output(wait=True)\n",
    "            display(print(' '.join(KWS_Surah)),(f\"{response['message'][idx]:.2}   \",idx , surah_que_main[idx] ))\n",
    "            time.sleep(0.1)\n",
    "            \n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    input_stream.stop_stream()\n",
    "    input_stream.close()\n",
    "    p.terminate()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KWS_Surah = []\n",
    "for i in range (4,8):\n",
    "    KWS_Surah.append(colored(surah_que_main[i],'red'))\n",
    "print(' '.join(KWS_Surah))\n",
    "\n",
    "p = pyaudio.PyAudio()\n",
    "input_stream = p.open(format=FORMAT,\n",
    "                        channels=CHANNELS,\n",
    "                        rate=RATE,\n",
    "                        input=True,\n",
    "                        frames_per_buffer=CHUNK)    \n",
    "MAX_LEN=30\n",
    "audio_surah_que = deque(maxlen=MAX_LEN)\n",
    "\n",
    "try:\n",
    "    counter = 0\n",
    "    recitated_word_counter=4\n",
    "    while True:\n",
    "        print(Fore.GREEN, 'Recording...' , Fore.RESET,Back.RESET )\n",
    "        while len(audio_surah_que) != MAX_LEN:\n",
    "            data = input_stream.read(CHUNK)\n",
    "            audio_surah_que.append(data)\n",
    "        else:\n",
    "            arr = np.frombuffer(b''.join(audio_surah_que),dtype=np.float32)\n",
    "            audio_output_path = './surah_audio_output_sample.wav'\n",
    "            sf.write(audio_output_path,arr,16_000)\n",
    "            \n",
    "            response = KWS_request(audio_output_path,'LSTM')\n",
    "            os.remove(audio_output_path)\n",
    "            audio_surah_que.clear()\n",
    "            idx=np.array(response['message']).argmax()\n",
    "            \n",
    "            if idx == recitated_word_counter and response['message'][idx] > 0.7:\n",
    "                \n",
    "                KWS_Surah[recitated_word_counter-4] = colored(surah_que_main[idx],'green')\n",
    "                recitated_word_counter+=1\n",
    "            \n",
    "            clear_output(wait=True)\n",
    "            display(print(' '.join(KWS_Surah)),(f\"{response['message'][idx]:.2}   \",idx , surah_que_main[idx] ))\n",
    "            time.sleep(0.1)\n",
    "            \n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    input_stream.stop_stream()\n",
    "    input_stream.close()\n",
    "    p.terminate()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## streaming and websockets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell is to test mic and server are running correctly. \n",
    "\n",
    "import websocket\n",
    "import _thread\n",
    "import pyaudio\n",
    "from colorama import Fore , Back\n",
    "\n",
    "import pyaudio\n",
    "FORMAT = pyaudio.paFloat32  \n",
    "CHANNELS = 1  \n",
    "RATE = 16000\n",
    "CHUNK = 1600  \n",
    "\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "#chunking parametrers\n",
    "MAX_LEN=30\n",
    "OVERLAP=25\n",
    "global chunks_counter\n",
    "chunks_counter = 0\n",
    "\n",
    "\n",
    "def on_message(ws, message):\n",
    "    print(Back.GREEN,message,Back.RESET)\n",
    "def on_error(ws, error):\n",
    "    print(Back.RED,'Error: ',error,Back.RESET)\n",
    "def on_close(ws, close_status_code, close_msg):\n",
    "    print(Back.RED , Fore.YELLOW,'CLOSED' , Back.RESET,Fore.RESET)\n",
    "def on_open_streaming(ws):\n",
    "    \n",
    "    def run(*args):\n",
    "        input_stream = p.open(format=FORMAT,\n",
    "                        channels=CHANNELS,\n",
    "                        rate=RATE,\n",
    "                        input=True,\n",
    "                        frames_per_buffer=CHUNK)    \n",
    "        try:\n",
    "            print('recording...')\n",
    "            while True:\n",
    "                data = input_stream.read(CHUNK)\n",
    "                ws.send(data ,  websocket.ABNF.OPCODE_BINARY)     \n",
    "        except KeyboardInterrupt:\n",
    "            ws.close()\n",
    "            input_stream.stop_stream()\n",
    "            input_stream.close()\n",
    "            p.terminate()\n",
    "        \n",
    "    _thread.start_new_thread(run, ())\n",
    "\n",
    "def on_open_chunking(ws):\n",
    "    \"\"\"\n",
    "    This function is used for chunking the audio data and sending it to the server.\n",
    "    \n",
    "\n",
    "    Args:\n",
    "        depends on global variables\n",
    "        MAX_LEN (int) : The maximum number of audio chunks to consider \n",
    "        OVERLAP (int) : number of chunks to shift i.e. the number of new audio chunks to add to queue\n",
    "    \"\"\"\n",
    "    def run(*args):\n",
    "        input_stream = p.open(format=FORMAT,\n",
    "                        channels=CHANNELS,\n",
    "                        rate=RATE,\n",
    "                        input=True,\n",
    "                        frames_per_buffer=CHUNK)    \n",
    "        try:\n",
    "            print('recording...')\n",
    "            que = deque(maxlen=MAX_LEN)\n",
    "            global chunks_counter\n",
    "            chunks_counter=0\n",
    "            while True:\n",
    "                data = input_stream.read(CHUNK)\n",
    "                que.append(data)\n",
    "                chunks_counter+=1\n",
    "                \n",
    "                if  chunks_counter % OVERLAP ==0 and len(que) == MAX_LEN:\n",
    "                    chunks_counter=0\n",
    "                    desired_amount_of_audio = b''.join(que)\n",
    "                    \n",
    "                    ws.send(desired_amount_of_audio,  websocket.ABNF.OPCODE_BINARY)\n",
    "                    \n",
    "        except KeyboardInterrupt:\n",
    "            ws.close()\n",
    "            input_stream.stop_stream()\n",
    "            input_stream.close()\n",
    "            p.terminate()\n",
    "        \n",
    "    _thread.start_new_thread(run, ())\n",
    "    \n",
    "\n",
    "\n",
    "def init_websocket_connection(endpoint:str , streaming:bool = True):\n",
    "    \"\"\"creating websocket connection\n",
    "\n",
    "    Args:\n",
    "        endpoint (str): endpoint \n",
    "        streaming (bool): streaming or chunking mode. changes the 'on_open' function to plug\n",
    "    \"\"\"\n",
    "    websocket.enableTrace(False)\n",
    "    ws = websocket.WebSocketApp(f'ws://127.0.0.1:8000/{endpoint}',\n",
    "                                on_open=on_open_streaming if streaming else on_open_chunking,\n",
    "                                on_message=on_message,\n",
    "                                on_error=on_error,\n",
    "                                on_close=on_close)\n",
    "    ws.run_forever()\n",
    "    \n",
    "    \n",
    "def broadcast_application():\n",
    "    init_websocket_connection('broadcast' , True)\n",
    "def streaming_application():\n",
    "    init_websocket_connection('audio_stream' , True)\n",
    "def chunking_application():\n",
    "    init_websocket_connection('audio_chunks' , False)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunking_application()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_application()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
